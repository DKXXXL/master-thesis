We implement our language design as a Coq plugin.
Rather than modifying the Coq kernel, a prototypical implementation as a
plugin allows a clearly defined trusted base and
improved compatibility with different versions of Coq.

The implementation works by translating programs in \Lang syntax into
programs that can be checked and evaluated by Coq.
The translation is compatible with interactive theorem proving, in that
a family is translated piece by piece, allowing each field to be defined
and checked separately.
The translation is modular and efficient, in that
code compiled for non-further-bindable fields of a base family
is shared with derived families without having to be rechecked.

\noindentparagraph{Explicit self parameterization.}

The spirit of the translation is to take ``family polymorphism''
literally: every field is translated into a Coq definition that is
polymorphic to (i.e., universally quantified over) a representation of
its enclosing family.
While this universal quantification has been implicit with the \Lang
syntax, it has to be made explicit in the translated Coq code.

\cref{fig:stlc-compiled,fig:stlcfix-compiled} illustrate the translation of the
\lsti{STLC} and \lsti{STLCFix} families from \cref{fig:stlc-mechanized}.
Fields of a family are translated into parameterized Coq modules
(or parameterized module types).

For instance,
consider field \lsti{env} in family \lsti{STLC}.
It is translated into a top-level module named \lsti{STLC\_env}.
This module has a parameter called \lsti{self} representing the enclosing
family: fields of the current family in the context of \lsti{env} can be
referenced through \lsti{self}.
In particular, \lsti{env} is defined as \lsti{id}\,->\,\lsti{option ty},
where \lsti{ty} is a late-bound reference to the \lsti{ty} field of the
enclosing family.
Hence, this reference to \lsti{ty} is translated in \lsti{STLC\_env} to \lsti{self.ty},
which is manifestly polymorphic to the enclosing family.
This translation of the \lsti{env} field can be shared
with a derived family even if it extends \lsti{ty} (e.g., \lsti{STLCProd})---%
no recompilation is needed because \lsti{self.ty} is not tied to a
particular definition of \lsti{ty}.

The type of \lsti{STLC\_env}'s \lsti{self} parameter is \lsti{STLC\_env\_Ctx},
a module type constructed from the translation of \lsti{env}'s preceding
field, \lsti{STLC\_ty}, and its context \lsti{STLC\_ty\_Ctx}.
In turn, \lsti{STLC\_ty\_Ctx} (not shown in \cref{fig:stlc-compiled}) is
constructed from the translation of \lsti{ty}'s preceding field,
\lsti{STLC\_subst}, and its context \lsti{STLC\_subst\_Ctx}.
Thus, the \lsti{self} parameter can be used to reference those and only
those fields in the current field's typing context, which echoes the
discussion in \cref{sec:sound}.

\input{stlc-compiled.tex}
\input{stlcfix-compiled.tex}

\noindentparagraph{Translating extensible inductive types.}

An \lsti{FInductive} definition is translated to a parameterized module type.
Consider the inductive type \lsti{tm}.
In \lsti{STLC}, it is translated to a top-level module type
\lsti{STLC\_tm} that declares a \lsti{tm} type,
its constructors (\lsti{tm_unit} etc.),
its partial recursor (\lsti{tm_prect_STLC}),
and the propositional equalities on the partial recursor (\lsti{tm_unit_eq_STLC} etc.).

Importantly,
\lsti{STLC\_tm} merely declares the existence of these names and their types;
it does not specify their definitions or proofs.
Knowing the existence of these names and their types
suffices to type-check and translate the rest of the family.
Leaving the definitions and proofs open enables \lsti{STLC} and \lsti{STLCFix}
to instantiate \lsti{tm} differently upon \lsti{End}~\lsti{STLC} and \lsti{End}~\lsti{STLCFix}.

The command \lsti{FInductive tm : Set += tm_fix : }\dadada~in family
\lsti{STLCFix} is again translated to a module type \lsti{STLCFix\_tm} (\cref{fig:stlcfix-compiled}).
It includes all names declared by \lsti{STLC\_tm}, and additionally declares
the new constructor \lsti{tm_fix} and related equalities.


\noindentparagraph{Translating recursion and induction.}

An \lsti{FRecursion} definition is translated in two parts:
first a module containing the definitions of all the case handlers,
and then a module type declaring the existence of the recursive function as well as
its propositional equalities.

Consider the translation of \lsti{subst} in family \lsti{STLC}.
First, a module named \lsti{STLC\_subst\_Cases} is generated interactively:
every time the programmer completes a \lsti{Case}, a case handler (e.g., \lsti{Def subst\_tm_unit})
is generated and added to the module.

Upon \lsti{End}~\lsti{subst}, a module type named \lsti{STLC\_subst} is generated.
%
As discussed in \cref{sec:latebind}, \lsti{subst} can be further bound,
so its definition is not exposed to the succeeding fields.
Accordingly, the translation \lsti{STLC\_subst} merely declares the type of \lsti{subst}
and the existence of its propositional equalities, leaving \lsti{subst} undefined
and the equalities unproven.
%
The propositional equalities are stated in terms of the case handlers,
whose definitions \emph{are} available through the \lsti{self} parameter.
For example, the type of \lsti{subst_tm_unit_eq} can be simplified to
\lsti{\forall x t, self.subst self.tm_unit x t = self.tm_unit}.
The equalities will be available for use in the checking and translation of
the succeeding fields through their \lsti{self} parameters.

Importantly, code generated for the case handlers is shared with derived families.
In \cref{fig:stlcfix-compiled}, \lsti{STLCFix\_subst\_Cases}
reuses---without rechecking---all the case handlers in
\lsti{STLC\_subst\_Cases} via the command \lsti{Include STLC\_subst\_Cases(self)}.

The translation of \lsti{FInduction} is similar, except that there is no need to
register propositional equalities, as \lsti{FInduction} proofs are considered opaque.


\noindentparagraph{Translation of further-bindables vs.\ non-further-bindables.}

In family \lsti{STLC},
field \lsti{env} and the case handlers for \lsti{subst}
are not further-bindable by derived families.
In contrast, \lsti{tm}, \lsti{subst}, and the related equalities
can be further bound.
The distinction is reflected in the translations.
The further-bindable fields are translated to module types that export only types of the fields.
The non-further-bindable fields are translated to modules that export definitional equalities
on the fields.
Opaque definitions can be further bound, too (\cref{sec:override});
they are translated to modules that export opaque definitions.

%\footnotetext{%
%Axioms in Coq module types are more like parameters than real axioms.
%They become part of the signature described by the module type.
%}


\noindentparagraph{Eliminating \lsti{self} by aggregation.}

Upon the conclusion of a family definition (e.g., \lsti{End}~\lsti{STLCFix}),
\lsti{self} parameters in the translations of the fields must be instantiated,
so that references like \lsti{STLCFix.typesafe} can be translated.
This is done by assembling the \lsti{self}-parameterized pieces into a single module.
%
For example, module \lsti{STLC} in \cref{fig:stlc-compiled} is generated
upon \lsti{End}~\lsti{STLC}.
All the fields are added to this module in the same order as they appear in the family definition.

For the non-further-bindables, the translated modules are directly included
(e.g., \lsti{Include STLC\_subst\_Cases} and \lsti{Include STLC\_env}).
%
The instantiations of \lsti{self}s are implicit thanks to a Coq nicety:
when including a higher-order module, Coq automatically instantiates its
parameter with the current interactive module environment.
For instance, command \lsti{Include STLC\_subst\_Cases} is successfully executed,
because Coq automatically instantiates \lsti{self} using the current
module environment, which contains all the fields required by \lsti{STLC\_subst\_Cases\_Ctx}.

For the further-bindables, \lsti{Axiom}s declared in the module types must be instantiated.
%
\begin{itemize}[align=left,itemsep=0pt,labelsep=*,leftmargin=*]

\item
An inductive type \lsti{tm} is generated, instantiating the axiomatized
\lsti{tm} type and its constructors.
%
The partial recursor \lsti{tm_prect_STLC} is defined
%to \lsti{\lambda}\,\lsti{P, tm_rect (\lambda}\,\lsti{t, option (P t))},
with the help of \lsti{tm_rect}, the recursor Coq generates for \lsti{tm}.
Proving the equalities on \lsti{tm_prect_STLC} is then trivial, by \lsti{reflexivity}.

\item
Similarly, \lsti{subst} is instantiated by applying the recursor \lsti{tm_rect}
to the (already included) case handlers (\lsti{subst\_tm_unit}, etc.).
Propositional equalities on \lsti{subst} are then trivial, by \lsti{reflexivity}.

\end{itemize}

Module \lsti{STLCFix} in \cref{fig:stlcfix-compiled} is generated upon
\lsti{End}~\lsti{STLCFix}, in the same fashion described above for \lsti{STLC}.
The translation makes code and proof sharing evident.
In particular, case handlers compiled for \lsti{STLC}
are reused to instantiate \lsti{subst}, \lsti{substlem}, and alike.
\lsti{STLC\_env} and \lsti{STLC\_typesafe}
are also reused to construct module \lsti{STLCFix}.
%
One may argue that because the first four constructors of \lsti{tm} are
repeated in \lsti{STLCFix}, the translation does not satisfy the
modular compilation requirement.
We could address this issue by using wrapper types,
but we consider restating constructors a small price to pay in return
for the clarity and concision of implementation.
We emphasize that compiled case handlers
are entirely reusable without rechecking, even with restated constructors.

Finally, the reference \lsti{STLCFix.typesafe} (where \lsti{STLCFix} is a family)
can simply be translated to \lsti{STLCFix.typesafe} (where \lsti{STLCFix} is a Coq module),
as the last line of \cref{fig:stlcfix-compiled} shows.

\ifShowOldWriting

\newpage

In this section, we describe how we can implement the proposed Vernacular commands.

After considering the pros and cons, we decide to implement a Coq Plugin where we can add new Vernacular commands and translate each new command into a bunch of Coq commands (the orignal surface Vernacular command) on the fly, instead of modifying the code-base of Coq. 

Despite possible difficulties for future maintenance, this approach has
various advantages: 1. it is the easiest and the most accessible way to
prototype as it relieves us the necessity of familarity of Coq base,
especially for the implementation of the module and functor; 2. we have
a clear definition of trusted-base---the whole Coq; 3. it is easy to
debug---we just need to check the translated commands; 4. it can be
well-incorporated with the existent tools like VSCoq; 5. it is more
accessible for the interested audience who can then easily adapt our
plugin and give a try---otherwise they have to download and re-build the
whole customized Coq; 6. it is more stable because the surface syntax of
Coq should be stable across different versions; 7. still, this plugin
can capture the key ingredients of implementing family polymorphism
inside Coq and act as a reference for guiding an appropriate
implementation of Family Polymorphism inside all sorts of proof
assistants.\YZ{Does the Coq implementation perform type checking to
prevent illegal uses fam poly?}\EDJreply{Yes. Because I define feature of fampoly as a direct translation into Coq feature.  The fampoly is like a shorthand into multiple Coq commands. \\
Do you find translation from my current metatheory into MLTT (without linkage) helpful? \\
Actually I am also not sure what this question is aiming towards. \\
Are you complaining there is no formal specification of my fampoly feature in paper and thus there is no proof my Coq surface syntax is related to metatheory? If so, I thought our Coq plugin is only advertising how powerful Fampoly is to solve expression problem in mechanized proving. I can decouple the relationship between our plugin and our metatheory if you find the gap is really there... And I can only claim metatheory is about a partial result on incorporating fampoly with dependent type, Coq plugin is using two example to illustrate how mechanized proving can get benefit and our Coq plugin is only inspired by our metatheory (and not claim any relationship). Doing so would require rearrange the metatheory section before this section. \\
But I don't think the gap is there at all because if you want all possible gaps to disappear, then the action of using debruijn is wrong because we need to prove the one using debruijn and the surface syntax that is not using debruijn are ``equivalent''. What's more, module is not sigma type because Module is not a term in Coq and thus there is no formalism of Module at all. Let alone I don't see the plausibility of mechanizing dependent type without debruijn. \\ 
Please comment back so that I know what corresponding modification is needed on the paper. 
}\YZreply{My original question was more directed towards the fact
that currently type checking is deferred to the Coq base. Generally,
when one uses a statically typed language as a compilation target, they
also want to have type checking at the source level, (1) because there
could be type errors at the source level that could not otherwise be
caught at the target level and (2) because it allows better error messages.
So it needs to be addressed why deferring type checking to Coq is OK.}\EDJreply{Good point. I think (1) is largely mediated by the fact the semantic of family is very close to that of the module. And the supporting feature is simple enough that there won't appear some problems like type error at source-level but not target level(2) is happening for sure but some basic error message is almost "transparent" i.e. the error-messages from Coq can be directly interpreted for the user. But my justification is solely empirical and as you know the empirical study for this paper is not really strong enough to support anything.}


\textbf{Family compiled into module.} The main idea of our plugin is to support family by compiling
family components into Coq modules, family ``types'' into module types, and the context of a given term
into parameters of the module. 

Since by default every field is inheritable, for each defining field (informally) $ .. \cL\sigma \vdash t : T $, we need to compile $t$ using
``universal-quantifier-wrapped term'' : $.. \vdash \lambda t : \forall
\cL \sigma. T$, so that field $t$ can be inherited to all the future family with strengthen context $\sigma^+$ with the help of weakening from $\sigma^+$ to $\sigma$.  However, instead of using Coq's universal quantifier, we choose module and module parameters to achieve this wrapping---each
family type in the context will be one module parameter
with the compiled module type, and thus each field of the family will
compile into a functor parametrized by its context. Doing so we can also
get rapid feedback from type checking when the users are defining each
field interactively with Coq. Note that, when compiling the context during the  compilation of a default (non-overridable)  field, all the default (non-overridable) former fields will expose their definition in the context while overridable fields will only expose their types.
% Insert one pseudo-code example for explanation of the mechanism
\begin{figure}[!htb]
  \begin{minipage}[t]{0.30\linewidth}
\begin{lstlisting}[language=Coq, escapeinside={@}{@}]
Family A.
Field a : nat := 1.
Field b : a = 1 := eq_refl.
Overridable Field a' : nat 
                     := 1. 
Overridable pins {a'}
  Field b' : a' = 1 
          := eq_refl.
EndFamily.
\end{lstlisting}
  \end{minipage}%\YZ{Can the self's be omitted?}\EDJreply{Yes. But in our current plugin implementation it is not omitted. I think altering example is fine. If you think some small distance between example and the current implementation is fine, please comment back and I will remove all the self_}\YZreply{If omitting the self's poses no technical challenges, then I'd say it's fine to also omit them in this figure.}\EDJreply{Omitting self_ is easy and not much engineering effort is required. But omitting self_ will lead to `A.a' instead of `a' actually. So if we really want `a' as you last time mentioned, then a resolution is required and more engineering effort is required. But I don't see any technical difficulties. I have removed all the self, please check.}
  \begin{minipage}[t]{0.35\linewidth}
\begin{minted}[fontsize=\footnotesize]{Coq}
Module a_4 (self_A: EmptySig).
Definition a : nat := 1. End a_4.
Module Type a_6.
Include a_4. End a_5.
Module b_7 (self_A: a_6).
Definition b 
  : self_A.a = 1
  := eq_refl. End b_7.
Module Type b_9.
Include a_6. Include b_7. End b_9.
Module a'_10 (self_A: b_9).
Definition a' : nat := 1. End a_4.
\end{minted}
  \end{minipage}
  \begin{minipage}[t]{0.3\linewidth}
\begin{minted}[fontsize=\footnotesize]{Coq}
Module Type a'_12.
Include b_9. Include a'_10. End a'_12.
Module b'_16 (self_A: a'_12).
Definition b' : self_A.a' = 1 := eq_refl. End b'_16.
Module Type b'_19.
Include b_9. Parameter a': nat.
Parameter b': a' = 1. End b'_19.
Module test_20 (self_A: b'_19).
Definition test : nat := 1. End test_20.

Module A. (* Final Aggregation *)
Include a_4. Include b_7. Include a'_10.
Include b'_16. Include test_20. End A.
\end{minted}
  \end{minipage}
  \caption{Example Code and Plugin Translation}\label{fig:plugin-example1}
\end{figure}


% Concrete details of compilation of a family term data structure
Taking \cref{fig:plugin-example1} as an example: immediately after the user inputs the line "Field a : nat := ..", our plugin will
translate this statement into the "Module a_4", "Module Type a_6"
two components.
{The "EmptySig" is just an empty module type.} Then,
these modules and module types will be generated and type-checked by Coq
immediately. The compiled "Module a_6" is then part of the context of
"Field b" (as the module type of "self_A") for "b" to refer to, so that "b" knows "a"'s concrete definition. The
internal representation of family "A" is the list of these
compiled modules and module types. We can achieve incremental
checking as in \ref{chg:software-engineering}. This makes all kinds of inheritance plausible---for example, if we
want to override "Field a", we simply replace that compiled module with
the new module inside the list. 
% This compiled module type serves as an abstraction that enables overriding. 



After closing the top family, the user can access family "A" like it is a module. This is achieved by a complete compilation from a family to a module,
via aggregating the list of the modules in the internal representation:
the plugin will repeatedly using Coq's "Include" command as the "Module
A" as \cref{fig:plugin-example1} illustrates.
References to family "A" outside "A" will be compiled to references to
this Coq module "A".

Note that, in our example, we include two
functors without arguments---this is due to a peculiarity of Coq's
"Include": uninstantiated parameters will be automatically
instantiated by the appropriate fields in the surrounding defining
module.
For example, "b_7" requires a module of type "a_6". Inside module "A",
when "Include a_4" is done, we will have a field \mintinline{Coq}{a : nat := 1}
inside the current surrounding "A". Then when we
\mintinline{Coq}{Include b_7} without specifying the module parameter,
Coq will try to find fields in the surrounding module (i.e., module
"A") to instantiate \mintinline{Coq}{self_A: a_6}. This is satisfied by
the \mintinline{Coq}{a : nat} that is included earlier.
The resulting compiled module is just like mundane Coq's module and 
thus maintains all the computational mechanism from Coq, fulfilling
requirements in \ref{chg:software-engineering}.\YZ{
  Do I understand correctly that an alternative to individually
  compiling family members to modules is to compile a family as a whole,
  but this alternative would not satisfy the "incremental type checking"
  or "instant feedback" requirement?
}\YZreply{
  Looks like individually compiling family members addresses not only
  this requirement, but also the "self vs. consistency" challenge.
}\EDJreply{Yes. Indivially compiling family member can achieve both requirements}

\textbf{Implementing "Overridable" and Overriding Chain.}
We implement "Overridable" by simply annotating the corresponding field type with a mark and its dependency. Then when compute the context during the compilation of the term for that field, we make sure only all the dependee and the non-overridable field will expose its definition in the context. 

Most parts of the implementation is quite similar to the default fields, except that when inheriting an overridable field, we need to make sure all its dependee is inherited earlier.  For example, in \cref{fig:plugin-example1}, the module "b'_16" will compile "b'" in the context of "a'_12" where the concrete definition of "a'" is exposed since "b'" depends on "a'"; while the module "test_20" is compiled in the context "b'_19" only knowing the type of "a'" and "b'".

When aggregating the overridable fields into a module, since everything has concrete definition now, there won't be any problem---we just use the same way that aggregates the default fields.



% explaining inheritance
\textbf{Implementing Inheritance.}
To simplify the implementation, we can consider all families as
inheritance and the standalone families are extending the empty family.
Thus, we need to deal with only inheritance during interactive theorem proving.


Simiarly, we
use a list to encode the three kinds of inheritance data in the
implementation. The inheritance data will include the
information of (1) the ``type'' of the parent family, (2) a list of
operation indicating how each field of the parent will be dealt with
(either inherited or overriden), (3) the operations indicating newly
extended fields, and (4) the ``type'' of the children family.
The surface syntax for the three kinds of inheritance
are demonstrated in the exemplar \cref{fig:plugin-example3}. The
implementation of overriding, inheritance, and extension is achieved by
simply manipulating the internal representation of a family
correspondingly---we can swap the module in the list for overriding,
retain the module in the list for inheritance, and add new module into
the list for extension. The compilation from a family (internal
representation) to a module still acts the same.

Though inheritance is solely a first-order data rather
than a ``mapping'', it is still possible to \text{mixin} two inheritance
data---by carefully ``mix'' the internal list, we can compose the
inheritance. However, a good definition of mixin is still under
investigation. We will show one practical example of \textit{mixin}
later.\YZ{How about making mixin composition one of your Req's? Also
does any example in the paper illustrate how to do this?}\EDJreply{mixin
is not really formalized. There is no example illustrate it. The only
example is the STLC example at the very end and I just kind of
``mention'' it. I think here I just want to emphasize, inheritance as
first order data is still possible to support composition.}

Our implementation also makes sure the first attempt of defining "c"
failed successfully in \cref{fig:plugin-example3}---proving "c"
would require a concrete definition of "a", and our plugin rejects
\mintinline{Coq}{c : a = 1 := eq_refl} in a context where we
only know \mintinline{Coq}{a : nat}.



% \textbf{Implementing "Final" and "Sealed Family".}
% We implement "Final" by
% exposing the whole definition directly into the compiled module type.
% Doing so will prohibit overriding. Say, "bop", declared "Final" in
% \cref{fig:plugin-example3}, is not overridable.
% We require the users to manual decorate field as `Final' so that the
% plugin will proceed with this special treatment.
% %\YZ{This para is confusing: which kind of equality is used, propositional or judgmental? Where does 'Final' show up in the example?}\EDJreply{I rephrase it and remove the part on talking about meta-theory. Now everything is about judgemental so I don't have to emphasize judgemental. I wanted to make the example small but I guess I can add one `Final' statement into our example. Don't resolve it until I add it into Figure 2.}

% % \textbf{Special Fields: Overridable/Sealed Family Simulating Sigma Type.} 

% The implementation of sealed family is not so different from that of the normal family, but we need to deal with the compiled module type. The compiled module type is the aggregation of the type instead of the concrete definition of the fields.
% For example, the compiled module type of
% "MonotonicF" in \cref{fig:plugin-example3} will be "{f : nat -> nat; p : monotonic f}". Thus any future overriding family will always have these two fields "f, p" with the corresponding type.\YZ{Can an overriding, sealed family define new fields not in its parent?}\EDJreply{I don't find this a problem but I didn't support it in my plugin. }


% The implementation of sealed family is not so different from that of the normal family, but we need to deal with the compiled module type. The compiled module type is the aggregation of the type instead of the concrete definition of the fields.
% For example, the compiled module type of
% "MonotonicF" in \cref{fig:plugin-example3} will be "{f : nat -> nat; p : monotonic f}". Thus any future overriding family will always have these two fields "f, p" with the corresponding type.\YZ{Can an overriding, sealed family define new fields not in its parent?}\EDJreply{I don't find this a problem but I didn't support it in my plugin. }

% Basically, {a : int -> int; property : a is monotonic}, in this case
%   we may want to override "a" using different computation, but we still wnat
%   this refinement/property




\textbf{``Extensible'' Inductive Type and Defining Recursor/Eliminator.}
We support extensible inductive types in Coq mentioned in
\ref{chg:extensible-inductive-type}.
% \footnote{Because to give a satisfactory
% formal definition and semantic for extensible inductive type is not
% something this paper aims at, and obviously requires more theoretical
% research effort}
%\YZ{I do consider that your plugin supports extensible inductive types. It's just that they are implemented as a plugin, rather than in the Coq core.}\EDJreply{I rephrased it. It is more like I don't think this is really the final form of extensible inductive type that everyone is happy about}

% \begin{figure}[!htb]\YZ{Change this one into STLC example, to see the layout}
%   \begin{minipage}[t]{0.32\linewidth}
% \begin{lstlisting}[language=Coq,  escapeinside={@}{@}]
% Family B.
%   FInductive b : Set 
%     := tt : b | ff : b.
%   Family neg_handler. 
%     Field tt : self_B.b 
%       := self_B.ff.
%     Field ff : self_B.b 
%       := self_B.tt.
%   EndFamily.
%   FRecursor neg 
%     about self_B.b 
%     motive (fun _ => self_B.b)
%     using self_B.neg_handler
%     by _rec.
%   Field example := 
%     self_B.neg self_B.tt. 
% EndFamily.
% Family B2 extends B.
%   Extend FInductive b : Set 
%     := uu : b.
%   Extend Family neg_handler.
%     Field uu : self_B2.b 
%       := self_B2.uu.
%   EndFamily. 
% EndFamily.
% \end{lstlisting}
%   \end{minipage}
%   \begin{minipage}[t]{0.65\linewidth}
% \begin{minted}[fontsize=\footnotesize,escapeinside=@@]{Coq}
%   (* Abstraction for Inductive Type *)
% Module Type b_3 (self_B: EmptySig_4).
% Parameter (b : Set). Parameter (tt ff : b). End b_3.      
% Module Type b_6 := b_3 EmptyMod.
%   (* Recursor to Set for b *)
% Module b_rec_12 (self_B: b_6).
% Definition __recursor_type_b_rec :=
%   forall P : self_B.b -> Set,
%   P self_B.tt -> P self_B.ff 
%   -> forall __i : self_B.b, P __i. End b_rec_12. 
%   (* Field B.neg_handler.ff *)
% Module ff_24 (self_B: b_6) 
%              (self_neg_handler: tt_23 self_B).
% Definition ff : self_B.b := self_B.tt. End ff_24.
%   (* Intermediate Module solely for type checking *)
% Module do_tc (self_B: neg_handler_26).
% Include b_rec_12 self_B.
% Parameter (recursor_for_type_checking : __recursor_type_b_rec).
% Definition term_for_type_checking :=
%   recursor_for_type_checking (fun _ : self_B.b => self_B.b)
%     self_B.neg_handler.tt
%     self_B.neg_handler.ff.  
% End do_tc.
% \end{minted}
%   \end{minipage}
% \caption{Example Code for Inductive Type}\label{fig:plugin-example2}
% \end{figure}


% \begin{figure}[!htb]
%   \begin{minipage}[t]{0.32\linewidth}
% \begin{lstlisting}[language=Coq,  escapeinside={@}{@}]
% Family STLC.
%   FInductive tm : Set := tm_var : ... 
%   FScheme tm_prec PRecT about self__STLC.tm.
%   FScheme Comp about self__STLC.tm_prec.
%   (* Generate Partial Recursor 
%   and the computational axiom *)
%   Family subst_internal. 
%   Final Field tm_var : ...
% := fun s x t => 
%     if (eqb x s) 
%     then t else (tm_var s).
% (* ... other three cases *)
%   EndFamily.
%   FRecursor subst 
%     about tm 
%     motive (fun _ => id → tm → tm)
%     using subst_internal
%     by _rec.
% (* Field test := (subst (tm_var 0) 0 (tm_var 0)).@\YZ{Why is "test" commented out?}\EDJreply{Because I want to show I can do it but I don't really want to d -- if I really do it the compilation result shown later will need to have this information.}@ *)
% (* ... *)
% EndFamily.
% Family STLC_bool extends STLC.
% (* ... *)
% Extend FInductive tm : Set :=
%   | tm_true : tm ...
% Extend Family subst_internal.
% Final Field tm_true : ...
%   := fun x t => tm_true.
% (* ... *)
% EndFamily. 
% Inherits subst.
% (* ... *)
% EndFamily.
% \end{lstlisting}
%   \end{minipage}
%   \begin{minipage}[t]{0.65\linewidth}
% \begin{minted}[fontsize=\footnotesize,escapeinside=@@]{Coq}
% Module Type STLC_Interface.
%   Parameter (tm : Set).
%   Parameter (tm_var : id → tm).
%   Parameter (tm_abs : id → tm → tm).
%   (* ... and all the fields defined before
%   and the abstraction for inductive type ...*)
% End STLC_Interface.
% (* Collect eliminator to Set for tm *)
% Module tm_rec_type (STLC: STLC_Interface).
%   Definition __recursor_type_tm_rec :=
%     forall P : STLC.tm -> Set,
%     (forall n : id, P (STLC.tm_var n)) ->
%     (forall (n : id) (i : STLC.tm),
%     P i -> P (STLC.tm_abs n i)) ->
%     ... -> forall i : STLC.tm, P i.
% End tm_rec_type.
% (* Compiled Field STLC.subst_internal.tm_var *)
% Module tm_var_handle (self__STLC: STLC_Interface)
%   (self__subst_internal: EmptySig_84).
% Definition tm_var :
%   forall (s : ident) (x : ident) (t : self__STLC.tm), self__STLC.tm :=
%   fun s x t => if eqb x s then t else self__STLC.tm_var s.
% End tm_var_handle. (* ... and more *)
% (* Assembling STLC.subst_internal when compilation *)
% Module subst_internal_410 (STLC: STLC_Interface). 
%   Module subst_internal.
%   Include tm_var_handle STLC. Include tm_abs_handle STLC.
%   Include tm_app_handle STLC. Include tm_unit_handle STLC.
%   End subst_internal.
% End subst_internal_410.
% Module do_tc (STLC: STLC_Interface).
% (* Intermediate Module solely for type checking *)
%   Include tm_rec_type STLC.
%   Parameter (recursor_for_type_checking : __recursor_type_tm_rec).
%   Definition term_for_type_checking :=
%     recursor_for_type_checking (fun _  ⇒ id → STLC.tm → STLC.tm)
%       subst_internal_410.tm_var ... .
% End do_tc.
% (* Ctx that expose concrete inductive type *)
% Module Type ctx_subst_113.
% Inductive __internal_tm : Set := ... 
% Include subst_internal_410. (* ... and more inclusion *)
% End ctx_subst_113.
% (* Compilation of recursor, not reusable
%     when inductive type extended due to the ctx *)
% Module subst_128 (self__STLC: ctx_subst_113).
% Include __motive_of_subst_107 self__STLC.
% Definition subst :=
%   self__STLC.__internal_tm_rec __motiveTsubst
% 	self__STLC.subst_internal.tm_var ... .
% End subst_128.
% (* Assembling the whole STLC *)
% Module STLC.
% Include tm_5. Include tm_prec_72. Include tm_prectm_81.
% Include subst_internal_410. Include subst_128.
% End v_129.
% (* Assembling STLC_bool.subst_internal via reusing 
%     most of the former compilation results *)
% Module subst_internal_248 (STLC_bool: STLC_Interface).
% Include tm_true_197 STLC_bool. Include ...
% (* The Reused Compilation result *)
% Include tm_var_handle STLC_bool. Include ...
% End v_248.
% @\YZ{It'd be useful to (1) show how the parts of the STLC family are assembled and recursor inserted, and (2) show the code compiled for STLC\_bool and how code from STLC is shared with STLC\_bool.}@
% \end{minted}
%   \end{minipage}
% \caption{Exemplar STLC Code, especially about Inductive Type}\label{fig:plugin-example2}
% \end{figure}
\include{ext-ind-type-compilation-example.tex}


% We start with the programming interface. We use "FInductive" to define an extensible inductive type and in the children family we use "Extend Finductive ..." to extend the type with new constructors. In \cref{fig:plugin-example2}, we make the example of boolean and three-valued boolean. 



% Recall, to construct a recursor, we need to create for each
% constructor a handler (function) and aggregate them into a family (e.g. "subst_internal" in \cref{fig:plugin-example2}). We use "FRecursor" to declare a recursor (e.g. "subst") by specifying the motive, the aiming inductive type and the handler family. Once recursor is created it will look like any other function fields (e.g. "subst"). Here in \cref{fig:plugin-example2} we can apply the function "subst" to "tm_var 0" as a new field. The inheritance of recursor is mainly delegated by the inheritance of the handler family---if the user extend with new constructor (e.g. "tm_true"), then the inherited family (e.g. "subst_handler") will need to extend correspondingly (e.g. "subst_internal.tm_true"). If not, the plugin will error with ``Non-exhaustiveness Pattern'' when inheriting the field "subst".\EDJ{This paragraph is duplicating but I am not sure how to make it concise.}


We illustrate the usage and plugin implementation of inductive type and reasoning in the exemplar \cref{fig:plugin-example2}, the example of
"subst" on STLC. When we define the extensible inductive type "tm", all
the following fields (e.g., "subst_internal.tm_var") can only know "tm"
is a type (of "Set") and there are at least two constructors for it,
just like what we declare in ``the abstracted interface''---the compiled
module type "STLC_Interface". However, we will not export
%\YZ{export or put?}\EDJreply{Ok.
%I just figure out how my wording is confusing. put is absolutely correct
%but by put I want to mean ``export''. I think I use ``export'' in
%several places...}
the eliminator of the inductive type "tm" to "STLC_Interface", \textbf{since
the mere existence of the eliminator will assert that there are only two
constructors} in "tm", lead to an \textbf{non-extensible} "tm"! Thus the "STLC_Interface" will only include the ``constructor'' themselves in the form of parameter function. Doing so, all the subsequent fields (e.g. "subst_internal.tm_var" and the corresponding compiled "tm_var_handle") can be inherited to the
context where "tm" is extended with new constructors without re-typechecking (re-compilation). 

To construct recursive function on a given inductive type, we use "FRecursion". "FRecursion" is a syntax sugar with (1) a normal family that includes all the case handling as field members, and (2) a step of doing type-checking and exhaustiveness checking (as in \ref{chg:extensible-inductive-type}). By considering all the case handling gathered in a family, the extension of the case handling can be simplified as the extension of the family. In our example, the resulting handler family is "subst_internal_410". While to achieve type-checking, we require the signature of eliminator about "tm" to be exposed. Here we show one
exemplar extraction of the eliminator to \mintinline{Coq}{Set} as module
"tm_rec_type". This concrete eliminator will be used by our "FRecursion".
For example, in \cref{fig:plugin-example2}, after closing an "FRecursion" block, our plugin will feed Coq the module "do_tc" for doing type-checking with specified
handlers (e.g. "subst_internal_410") during compilation---what it does is
basically
assert the existence of the recursor and apply to see if handlers can
pass the type checking by Coq.\YZ{
  Sounds like we can still claim the plugin is doing type checking.
  It's just that the checking is done by generating some Coq code and
  then having Coq check it.
}\EDJreply{Yes! If my phrasing doesn't make things clear (i.e. our plugin is doing typechecking),
feel free to change the phrasing for me}


% Note that, at this point of the example,
% "FRecursion" will only do type check via "do_tc". 
% The compilation of "FRecursor"  only happens when closing the whole
% family, by including "subst_18". 
% However, notice the context (parameter) of the "subst_18": it is "ctx_subst_113", where the concrete inductive type is exposed in the context, instead of "STLC_Interface". This concrete exposition makes "__internal_tm_rec" accessible, but also makes "subst_18" non-inheritable when "tm" is extended. Thus in every children family, modules like "subst_18" needs to be regenerated.
% Thus a "FRecursion" field is unlike normal fields, 
% even though it is exposed as a plain
% function for the succeeding fields in the programming interface.

 

Note that, the "FRecursion" field (e.g. "subst") is not actually
inheritable, because it is not compiled like any other fields.
Recall how other fields are compiled into reusable pieces of
(parametrized) modules; a recursor field is not compiled like that. 

In our example "STLC", when the "FRecursion" block is closed, it will only do type check via "do_tc".  The compilation of "FRecursion"  
only happens when closing the top-level
family "STLC", by generating "subst_18" on-the-fly and assembly it into "module STLC". .
However, notice the context (parameter) of the "subst_18": it is "ctx_subst_113", where the concrete inductive type is exposed in the context, instead of "STLC_Interface". This concrete exposition makes "__internal_tm_rec" accessible, but also makes "subst_18" non-inheritable when "tm" is extended.
Thus, for all the children family, we have to regenerate module and replace the
module with the ``new'' recursor using ``the same'' handler
family (e.g. "Module subst_242", whose context "self__STLC_bool" is also different from that of the parent).

This is affordable since each aggregated recursor handler
doesn't need to be rechecked: look at surface syntax, the user only need to handle the new constructors like "tm_true" in "STLC_bool.subst"; during compilation,"Module subst_internal_248" will directly include the old handlers 
from the parent (e.g. "STLC.subst_internal.tm_var") that have already been type-checked (since they only rely on ``the abstracted interface''
module "STLC_Interface" and thus well-typed). 
Thanks to family inheritance, their corresponding compiled module from the parents---for example, 
the module "tm_var_handler"---doesn't require another type-check and is
re-used during the type-checking and compilation of "STLC_bool.subst".


Of course, in the implementation, these details are all hidden and when the user ``inherit'' the recursor "subst" in the surface syntax,
our plugin actually will actually do a second exhaustiveness
checking in children family mentioned above, with the ``same'' motive and the ``same'' handler family.
%\YZ{A source of confusion in this section is that the reader often has to guess who performs the actions: is it the programmer or the Coq plugin?}\EDJreply{Good idea. Let me reorgnaize this section to ``first programming interface'', then ``plugin implementation''}

The plugin realizes the "Extend FInductive" via overriding---%
when extending an inductive type $A$ with new constructor $c$, our
plugin will generate a new (syntactic) inductive definition $A'$ with
this new $c$ on the fly and feed it to Coq. But we must be careful on
the exposing data in the family type (the compiled module type) for the
succeeding field. In other words, we must be careful on the interface
(context) based on which the succeeding fields are defined.

 

% Insert one pseudo-code example for explanation of the mechanism
% Use the natural number example




% Explain why the module type need extra care

% explain how recursor is constructed
% explain we have the incremental-checking for any recursor 

% explain ftheorem wrapping this complicated recursor
% This design of decoupling of the case handlers and exhaustiveness
% checking handles the semantic well, however, it brings a lot of
% boilerplate code---we need to create an extra (handler) family, manually
% specifying the type of each case handlers, and then using "FRecursor" to
% ``tie the knot'' and construct a recursor field from the handlers.  


For theorem proving, we provide a command "FInductive" that is doing similarly as "FRecursion" in programming interface but using tactic programming. Like "FRecursion", "FInduction" can handle newly added constructor in the children family as well.

% This command (1) can avoid specifying the handlers family and the type of each case handlers when writing induction like above, (2) invoke
% proof interaction mode and thus allow tactic programming, (3) and is also open to extension like "FRecursor". We expect this "FTheorem" to be used in theorem proving but not general programming. 

To summarize, (1) to make each recursor
handler inheritable, we need to seal an abstraction around the inductive
type (e.g., module "STLC_Interface"); but to construct a recursor (and carry out the
exhaustiveness checking), we need to break this abstraction and see the
concrete definition of the inductive type (e.g., module "tm_rec_type"). Our
meta-theory need to handle these two seemingly contradicting ideas
simultaneously when solving \ref{chg:extensible-inductive-type}.
(2) \ref{chg:extensible-inductive-type} is also resolved by this
\textit{decoupling} of the syntax of implementation of recursor handlers
and exhaustiveness checking under the syntax sugar. 
The former is handled by family inheritance
and thus avoiding the boilerplate code; and the non-inheritance of the
compiled recursor ensures the exhaustiveness checking happens for every
recursor.  


% Attempt to rewrite the section for Overridable pins inductive/Closing Fact , and also introduce the concept of mixin

\paragraph{Tactics and propositional recursors.}

However, "FRecursor" provides barely enough functionality for
manipulating our(extensible) inductive type, and it is far from
satisfactory to be a legitimate programming interface. (1) We do not yet
have any computational information using this "FRecursion" until its
compilation into module, i.e. we cannot do computation with "subst" during
the construction of the family using the current compilation strategy.
(2) The necessary tactic like "discriminate" and "injection" is not
applicable on (extensible) inductive type.



\textit{Generating Scheme for Propositional Computation Axiom for FRecursor}. Thus we support "FScheme" to remedy problem (1): our plugin can automatically generate(postulate) the computational ``axiom'' for the specified  into the family and prove them during compilation into modules. For example, "FScheme Comp about subst" in \cref{fig:plugin-example2} will compile into "Module Type substtm_139", filled with all the computational axiom for "subst". There module type when included in the context can be used to reduce "subst"; they will also be proved after compilation.

However, it is only introducing computational information \textbf{propositionally} but not judgementally, and we might need the help of rewriting tactic on these computational ``axiom'' to justify the computation. 

% However, this method, though remedies problem (1) to some extent, is only introducing computational information \textbf{propositionally} but not judgementally. Thus, for example, during theorem proving, we might need the help of rewriting tactic on these computational ``axiom'' to justify the computation. And in the full dependent type theory, propositional equality is sometimes not enough and bringing a lot of coding challenges\footnote{For example, for "u : F a"， "v : F b", "u = v" is not even well-typed when "a = b" only propositionally hold. Transport is required during reasoning.}.

% To resolve (2), we can simply  Our following three sections will try to resolve these problems. 
% we notice that, the power of "FRecursor" comes from its distinction from the mundane fields and its ability to stab through the abstraction of "FInductive". However, the latter is quite similar on how "Overridable .. pins {..}" can see through the abstraction of former pinned overrdiable fields. This inspires us to generalize pins to not only apply to overrdiable fields, but also "FInductive" and "FRecursor" fields. 


\textit{Overridable pins Inductive Type and FRecursor}. We notice that,
the power of "FRecursor" comes from its distinction from the mundane
fields and its ability to stab through the abstraction of "FInductive".
Interestingly, the latter is quite similar to how "Overridable .. pins
{..}" can see through the abstraction of former pinned overrdiable
fields. This inspires us to generalize "pins"---we support pinning
inductive type and "FRecursor". Once "FInductive" and "FRecursor" fields
are pinned, we are directly exposed with the primitive coq-providing
recursor and \textbf{judgemental equality} of the computational
information for "FRecursor". 

The compilation mechanism is to simply make the underlying, compiled
definition of inductive type and recursor included as part of the
parameters of the module, just like how the concrete definition of
overridable fields are included inside the parameters when pinned. This
way, the computational information from vanilla Coq can be transmitted
into the context.  


% However, only overridable fields can pin the (extensible) inductive type and "FRecursor".

% Upon this design, it is natural to make "Overridable" fields to provide ``automatic overriding'' as it is likely the inherited tactic is able to provide a proof even in the context with the extended inductive type. 

% But a question arises: do we consider an ``automatic overridden'' field \textit{inheriting} the parent field or \textit{overriding} the parent field? 
% TODO: Mention still needing total recursor, for exhaustiveness checking


% To fulfill \ref{chg:software-engineering}, we need somehow to aggregate tactic inside compiled module type, due to the implementation of our family---when defining every field of 

% For example, 

%   For example, currently it is not possible to define a
% vanilla inductive type dependent on our ``extensible'' inductive type
% due to the invisibility, unless we compile the whole family. Similarly,
% customized tactic expressions as required by \ref{chg:software-engineering} cannot
% use lemmas or theorems in the fields of the current defining family as
% well.

% To remedy this, we support a "MetaData" command. This command
% bundles any original Coq primitive (in the current family
% context) into the compiled module type, and makes them visible to the
% following defining field. Doing so, we can define customized tactic as
% in \ref{chg:software-engineering} that uses lemmas proved in the family.


% Following needs a rewrite
To achieve "injection" in problem (2), we can easily generate/postulate
$n$ ``injection'' propositions for $n$ constructors, just like how we
did for computational information. But for "discriminate" tactic, this
way will generate $O(n^2)$ propositions in a verbose way. 

What's more, to manipulate inductive type, it is not necessarily enough
to only support "injection" and "discriminate" tactic (and their
corresponding axioms). Recall in vanilla inductive type, we believe that
eliminator is the strong-enough ``toolkit'' because the eliminator (with
its computational information) is the \textit{universal property} of the
inductive type---it extensionally describes the inductive type up to
(certain) isomorphism. This universal property is much more powerful
than "injection" and "discriminate" and maybe the most powerful tool we
can have to reason about inductive type. 

For example, given a new inductive type, to prove "discriminate" between two constructors, i.e., "c$_i$ .. $\neq$ c$_j$ .." for two different constructors, we simply use the eliminator to map each constructor to a different natural number, assuming we can "discriminate" different natural numbers.\footnote{Rigorously, to avoid any suspicious circular reasoning, we can use large elimination to map them to $\top$ and $\bot$, and because $\exists a : \_, \top$ holds for the former not the latter, thus $\top \neq \bot$.} "injection" can be supported in a similar way---we can simply let eliminator map each inductive type to the arguments of the constructors. 

Thus it is natural to think if (an altered form of) eliminator can be
postulated into our context, since apparently we cannot directly
postulate the original eliminator here---because the sole existence of
the eliminator prevents any possible future extension of the inductive
type. 

Thus we alter eliminators into \text{partial recursor}, such that (1) it
will not hinder the future extension, and (2) it is still powerful
enough to derive injectivity ("inject") and disjointness ("discriminate") of constructors.
What's more, (3) we can prove this \text{partial recursor} is
actually good enough because it can act as an \textit{extensional
characterization} for extensible inductive types \textit{to some extent}.
We will call it \textit{propositional} because we only propositionally
postulate its computational information for Coq.

\textbf{Propositional Partial Recursor.} Partial recursor is simply adding partiality into the signature of the eliminator, by wrapping each motive with "option". 
% This section is introducing the concept of \textit{partial recursor}. The motivations are various: (a) we want to have some semantic characterization of extensible inductive type. (b) We want to show that, some tactics reasoning like "inversion" is still valid even in extensible inductive type. (c) More generally speaking, notice that, for both "FRecursor" and "Closing Fact", they are not necessarily ``\textit{conservative extension}''---they will possibly disrupt arbitrary extension of inductive type. For example, for \cref{fig:STLC-example}, once we proved \mintinline{Coq}{var_not_value : ∀ i, ~ value (tm_var i)} in the "Family STLC" either using "FRecursor" or "Closing", then any children family of "STLC" cannot extend a new constructor \mintinline{Coq}{vvar: ∀ i, value (tm_var i)}. We want to see what leads to a conservative extension---and the partial recursor is helpful for this question.

% This raise an interesting question: what propositions are conservative extension? 
% For example, apparently we know injectivity of constructor, (i.e. \mintinline{Coq}{STLC.tt ≠ STLC.ff}) must be conservative because it holds for all extension of the inductive type "STLC.tm". But is there a \textbf{``best'' proposition} (i.e. the proposition that everything the proposition derives are exactly conservative extension)?

% Since injectivity of the constructors are derived by recursor in vanilla inductive type, an educated guess would be related to recursor. But we know the original recursor won't work because the mere existence of the original recursor will break extensibility. Thus we need to modify it.
% The simplest idea is to allow partiality---i.e., returning
% \mintinline{Coq}{option T} instead of the original \mintinline{Coq}{T}.

% We call this \textit{propositional partial recursor}.\YZ{What is propositional about partial recursors? I suppose it has to do with the computational axioms?}\EDJreply{Yes. I want to emphasize the equality used by computational axiom is not judgemental equality. So Coq cannot automatically compute/reduce when a partial recursor applied to a term. This reduction can only be semi-automatically done by using [rewrite tactic and the computational axiom].}
For example, the propositional partial recursor for \mintinline{Coq}{B.b} is

\begin{minted}[escapeinside=@@]{Coq}
(* Inside Family B *)
FInductive b : Set := tt : b | ff : b.
b_prec : forall R, option R -> option R -> b -> option R.
\end{minted}
with appropriate computational axioms.  This partial
recursor differs from original recursor only by decorating the return
type with an "option" and can thus be automatically generated (and
the case is similar for those computational axiom). Thanks to the
partiality, partial recursor doesn't break
extensibility (i.e., all future extensions of "b" can support this
"b_prec") because for example, "b_prec" can just return "None" when bump into extended constructor in the future. 

The second thing to check is that, if partial recursor is theoretically enough for injectivity of the constructor. The rough idea to achieve this
is to use "b_prec" to reflect our special inductive type into vanilla
inductive type, and use the original "injection" and "discriminate"
tactics. For example, we want to prove \mintinline{Coq}{B.tt ≠
B.ff}, we simply 
\begin{minted}{Coq}
Definition reflect : B.b → option bool 
                   := b_prec (fun _ => bool) (Some true) (Some false).
Definition tt_neq_ff : B.tt = B.ff → False.
(* Because B.tt = B.ff → reflect B.tt = reflect B.ff 
      → Some true = Some false    (by computational axiom)
      → False           (by discriminate of bool and injection of Some) *)
\end{minted}


Thirdly, even though we cannot prove partial recursors are \textit{the
best} reasoning tool for extensible inductive types,
the above "reflect" function implies that propositional partial
recursors are a good \textit{extensional characterization} of
(non-indexed) extensible inductive type (under certain
constraints)---because we can ``embed'' vanilla inductive types into
types supporting a partial recursor: "reflect" is actually a left
inverse of an injection "bool → self_B.b". A bit more formally, and
restricting our focus to non-indexed inductive types:

\begin{theorem}\label{thm:prec-complete} Given a list of $n$ pairs of types $\{ x : "A"_i \vdash "B"_i(x) \}_{i}$ s.t.


  \begin{itemize}
    \item \textlabel{(Detectable Partiality)}{prop:detectable-partiality} for each pair $x : A_i \vdash B_i(x)$,
    \begin{minted}{Coq}
      Axiom detectable:
      forall {T} {a} (f: Bᵢ a -> option T),
        {forall x, f x <> None} + {exists x, f x = None}.
    \end{minted} 
    \item Define $"C"$ as the inductive type using this list, i.e.,
    \begin{minted}{Coq}
Inductive C : Set := ... | cᵢ : forall (x : Aᵢ), (Bᵢ x -> C) -> C | ...
    \end{minted}
    \item Assume an arbitrary type \mintinline{Coq}{D : Set} with
    \begin{itemize}
      \item $n$ \textbf{functions} \mintinline{Coq}{dᵢ : forall (x : Aᵢ), (Bᵢ x -> D) -> D}  
      \item a partial recursor
      \begin{minted}[escapeinside=@@]{Coq}
prec : forall (R : Set), ..,
  (rᵢ : (forall (x : Aᵢ), (Bᵢ x -> option R) -> option R)), ..,@\YZ{Can R be indexed?}\EDJreply{This part actually currently incorrect and needs an extra constraint. Let me fix other parts first}\YZreply{Is this constraint Detectable Partiality?}@
      D -> option R
      \end{minted}
      \item and $n$ (propositional) computational axioms: for all $i$, 
      \begin{minted}{Coq}
prec {R} r₁ r₂ .. rₙ (dᵢ aᵢ bᵢ) 
    = rᵢ aᵢ (fun x => prec {R} r₁ r₂ .. rₙ (bᵢ x))
      \end{minted}
    % where \mintinline{Coq}{(lift rᵢ) : (forall (x : Aᵢ), (Bᵢ x -> option R) -> option R)} is defined as expected
    \end{itemize}
  \end{itemize}
  Then there exists an \textbf{embedding} from "C" to "D".
  More concretely, we can have \mintinline{Coq}{inj : C -> D} and 
  \mintinline{Coq}{linv : D -> option C} defined using the eliminator of\/
  "C" and "prec" of\/ "D" (both acting like identity) such that
  \begin{minted}[escapeinside=@@]{Coq}
    forall c : C, linv (inj c) = some c@\YZ{Does 'linv' correpsond to 'reflect' above? If so, then the reader should be explicitly reminded of this fact rather than having to guess it.}\EDJreply{Added below. I mentioned left inverse 'linv' is the generalization}@
  \end{minted}
\end{theorem}
The reason we have this weird looking \ref{prop:detectable-partiality}
is that the construction of "linv"---at one point, using "prec" to
construct "linv", we need to prove \mintinline{Coq}{∀ a, (Bᵢ a → option
C) → option C}, generally not provable, unless we have
\mintinline{Coq}{option (Bᵢ a → C)} as an argument instead.\YZ{Don't you really mean
"unless we have the Detectable Partiality assumption instead"?}
This \ref{prop:detectable-partiality} is exactly transforming former to
the latter:
recall that, according to the semantic of inductive type,
\mintinline{Coq}{(Bᵢ a → option C)} is actually the result of using
"linv" on the substructure of "D". This function points out each of the
translation result from "D" to "C" and some of them are "None" because
of failing. For us, we only care about if \textbf{any} of the
substructure of "D" fails to translate---if any substructure fails,
then the whole translation fails and thus we should have "None" as
return.

\ref{prop:detectable-partiality} is derivable for all the finitary branching inductive type\footnote{By simply enumerating through the finite domain "Bᵢ a"}---that includes all of the examples shown in this paper. 

\ref{prop:detectable-partiality} is not generally supported when using function (infinitary branching) in the constructors, for example, the case of using \textit{higher order abstract syntax} to represent functions in "tm". However, in those cases, we still have partial recursor provable and useful---we just cannot see the partial recursor as a strong toolkit anymore.


We emphasize that in \cref{thm:prec-complete}, each function $"d"_i$ can
actually be considered as a constructor, because they can be reflected
to real constructors of "C" using the left inverse "linv" (directly generalization of "reflect : self_B.b → option bool" of the above). Thus, \cref{thm:prec-complete}
implies that (1) every future extension of the inductive type can
support this partial recursion (trivially); (2) every type supporting
this partial recursor with its computational axioms at least supports
these constructors because of the embedding. In other words, \textit{a
type (at least) supports these constructors if and only if this type
supports the corresponding partial recursor (with the computational
axioms)}.  Thus, we can argue that the partial recursor gives a sound and
complete extensional characterization of all the extension of a given (non-indexed)
inductive type.\YZ{What about indexed inductive types?}\EDJreply{Done. I also have a formalization of this proof now, for indexed type and dependent eliminator. Though the formalization is about single constructor.}

We can support partial dependent eliminator as well, but partial dependent eliminator can deduce "prec" and our "prec" is enough to contruct this left inverse for (non-indexed) inductive type. 
\begin{minted}{Coq}
  pdelim : ∀ (P : D -> Set), .., 
  (rᵢ : (∀ (x : Aᵢ) (w : Bᵢ x -> D), (∀ (b : Bᵢ x), option (P (w b))) 
    -> option (P (dᵢ x w)))),  .., ∀ (d : D) -> option (P d)
\end{minted}
To generalize to indexed inductive type, we need to base on indexed W type~\cite{martin1982constructive, morris2009indexed,jashug2017} and dependent eliminator is unavoidable. Please refer to the supplementary material and appendix for a formalization of the left inverse in the context of indexed inductive type. 

Though partial recursor is provable by simply using "FRecursor", for convenience, we support using "FScheme" to automatic generate it. For example, "FScheme tm_prec PRecT" in \cref{fig:plugin-example2} can lead to compilation of "Module tm_prec_72". At the meantime, we implement a tactic "prec_discriminate" that when fed with the appropriate partial recursor (with computational axiom generated), it can discriminate an absurd equation between two different constructors of extensible inductive type.
\begin{minted}{Coq}
  FInductive tm : Set := tunit : tm | tvar : ident -> tm | ..
  FScheme tm_prec PRecT about self__STLC.tm.
  FScheme Rec2D about self__STLC.tm_prec.
  (* ... *)
  Goal ∀ i, tvar i = tunit -> False.
  intros i H. prec_discriminate self__STLC.tm_prec H. Qed.
\end{minted}


Finally, we should point out that, the mere postulation of partial recursor is \textit{conservative}. In other word, generally speaking, using "FRecursor" to prove theorems is not necessarily a ``\textit{conservative extension}''---the proved theorems might hinder future extensions of the inductive type. For example, for \cref{fig:STLC-example}, once we proved \mintinline{Coq}{var_not_value : ∀ i, ~ value (tm_var i)}\footnote{Achieved by three steps: first proving partial recursor using "FRecursor", then use partial recursor to prove "tm_var i $\neq$ tm_lam v body" and finally using "FRecursor" prove "var_not_value" by induction on "value"} in the "Family STLC" either using "FRecursor", then any children family of "STLC" cannot extend a new constructor \mintinline{Coq}{vvar: ∀ i, value (tm_var i)}. But the postulation of partial recursor will not cause this problem.


\textbf{Escape Hatches: "Meta-data" and "Closing Fact".} There are cases the developers want to refer to global reasoning or just escape from the programming paradigm of family polymorphism. Thus we provide the following mechanism.

\textit{"MetaData".} Due to our implementation of the family,
the family data is invisible to Coq internal and thus causes some
inconvenience. For example, in
\cref{fig:plugin-example-global-reasoning-meta-data} the first attempt
of defining tactic "inv" fails. Because the earlier defining "tm",
"value", and "value_not_app" are currently just data structure in the
plugin and not visible for Coq internal (only visible after the family
is compiled into module). 

Thus to refer to them, we have to scope the defining tactic with
"MetaData" and our plugin will set up the appropriate environment. After
a "MetaData" block is closed, the plugin will aggregate the content into
the family being defined so that they can also be used when defining
the following fields of the family.\YZ{It reads like this MetaData wrapping could be automatically inserted, no?}\EDJreply{Not sure. This auto-insertion will practically look like hijack the original Coq command. I am not sure this kind of hijacking can happen.}


This can not only fulfill \ref{chg:software-engineering} and define new customized tactic but also integrate any vanilla Coq's feature into family. For example, if the user don't want to use "FInductive" but only the non-extensible "Inductive" type, the user can simply declare the inductive type inside "MetaData" to be part of the family.


\textit{"Closing Fact".} We have described how to use partial recursor to "discriminate" different constructors of an extensible inductive type. Practically, "discriminate" is used when proving inversion lemma, like forementioned "var_not_value". 

However, it seems verbose to prove this simple inversion lemma using "FRecursor", let alone the fact this "FRecursor" will need to be extended every time the inductive type (e.g. "value") is extended. Thus we provide a command "Closing Fact". "Closing Fact" will prove the given lemma in the context where all the definitions \textbf{are exposed}. Thus the above "var_not_value" can be simply achieved by 
\begin{minted}[fontsize=\footnotesize]{Coq}
Closing Fact var_not_value : 
  forall i, ~ self__STLC.value (self__STLC.tm_var i) 
    by { intros i H; inversion H; eauto }.
\end{minted}
We can use "inversion" tactic here thanks to the exposure of the vanilla inductive type in the context. 

We simply support inheritance for "Closing Fact". However, we fix the semantic of the "Closing Fact" by this one-line \textbf{untyped} proof script---and thus we restrict "Closing Fact" to be used in a proof-irrelevant way.

\textit{"Overridable .. pins {..}" vs. "Closing Fact".} A keen reader will notice a great deal of similarity between "Overridable .. pins {..}" vs. "Closing Fact" as they both expose the concrete definition of the former fields (including extensible inductive type). In fact, using "Overridable" can prove this "var_not_value" in an almost identical way. The only difference seems to be the ``auto-inheritance'' of "Closing Fact" with the help of the proof script where even if the extensible inductive type is extended---but this difference seems superficial as we can implement something similar for "Overridable".

The main difference is that, the semantic of a "Closing Fact" field is the \textbf{untyped} proof script but the semantic of a "Overridable" field is always a \textbf{typed} proof term. Thus when a pinned extensible inductive type/former field is extended/overridden, we can still \textbf{inherit} the "Closing Fact" but we can only \textbf{override} the overridable field.

This difference might seem small and irrelevant to the user from the first sight but this becomes quite relevant when we introduce \textit{mixin} of two inheritances.
% \textbf{Overridable Field and Closing Fact}
% It is natural to consider the ``automatic overridden'' resulting inheritance, as we are using the same tactic (or the same untyped Gallina term to provide the proof). But for type theorists, these are definitely overriding because these are definitely different typed Gallina term! 

% Instead of making a decision here, we introduce "Closing Fact":  "Closing Fact" will make a field tied with the untyped proof script and only allows inheritance. Of course, during inheritance, the untyped proof script will always be re-run and introduce (potentially) \textbf{different} typed terms\footnote{Thus called "Closing Fact" as its semantic depends on the closed  surrounding family in a global sense}. Thus, we encourage the user to use it in a proof-irrelevant way, where the difference between typed terms can be ignored. Then, the ``automatic overridden'' will always lead to overriding. 

% This is similar to "FRecursor"---we only allow inheritance for "FRecursor" as its semantic can be considered as a tie between a handler family and an extensible inductive type, where during each inheritance, the tie will be rechecked. 

% Just like "Overridable .. pins {..}", "Closing Fact" provides the ability to stab through all the abstraction of the former fields, and thus has the similar functionality  as "Overridable .. pins {..}".

% \textbf{Mixin}. The reason we introduce two different semantics for two similar functionality for "Overridable ... pins {...}" and "Closing Fact" is due to the fact that, we want to introduce mixin in our facility. 



% \textit{Mixin} is implemented by making inheritance judgement/data supporting \textit{weakening} and composition.  
\textbf{Mixin} is implemented by making inheritance judgement/data supporting \textit{weakening} and composition.More concretely,  for an inheritance from parent family type $\sigma_1$ to children family type ${\sigma_2}$ and another inheritance from ${\sigma_1}$ to ${\sigma_3}$, we can weaken(expand the input of) the second inheritance to ${\sigma_2}$ then the output children family type becomes ${\sigma_3'}$. Then the ``composition'' of the very first inheritance and the altered second inheritance is the resulting mixin. 

However, this implementation breaks commutativity: for example, when the two inheritances are overriding a given field in different ways, the order of composition becomes important. Thus to avoid this ambiguity, we simply \textit{don't allow overriding fields} inside any inheritance when we do mixin. In this case, a proof irrelevant "Closing Fact" is a useful alternative for overriding.

Note that, every top level family can be considered as an inheritance because (1) our implementation is simplified by considering every family as an inheritance, and (2) we want to align with the complete family polymorphism advocated by \citet{zm2017} where the user can override the parent of the inheriting family (inside a family due to late-binding), and in these case the nested inheriting family is more a inheritance data than a ``standalone'' linkage. Thus we will store for every top level family in our plugin the corresponding inheritance data together with a ``reference'' to the parent family. 



% Mixin example:

% Then our mixin command can single out the inheritance data from "STLC_bool" and "STLC_prod" and ``mixin'' the inheritance to reach "STLC_bool_prod". 

% However, we don't allow any overriding happening in either mix-edin inheritance, because it will break the commutativity and complicate the semantic without much benefit. Overriding is not allowed in this case, but "FRecursor" and "Closing Fact" is still allowed since they only allow to be inherited. 




% \textbf{Hack: Global Reasoning.}\YZ{This is more like an escape hatch. Escape from family poly into vanilla Coq. Maybe these two hacks should be grouped into one section}
% % explain the places using ``Closing Fact''
% % 1. No need to export partial recursor
% % 2. Computational rule for the total recursor
% Unfortunately, there are cases an extensible proof brings more boilerplate code. For example, in STLC, we expect
% ``\mintinline{Coq}{~ value (tm_app x y)}'' to hold in all future
% extensions of STLC---applications will never be considered as values
% by any extension.
% We know the correct way to prove the proposition
% \mintinline{Coq}{value_not_app : value (tm_app x y) -> False.} is to use
% "FRecursor" to inductively reason "value (tm_app x y)". However, this
% means that every time we extend "value" with new value forms, we will
% need to extend corresponding inductive cases for "app_not_value". We can
% imagine the newly extended proof to be boring case analysis that the
% newly added value forms are not "tm_app". What's worse, similar
% scenario can happen on other statements like \mintinline{Coq}{~ value
% (tm_if cond x y)}. Everything here is extended correctly but we end up
% having boilerplate code, \textbf{semantically}.
\begin{figure}[!htb]\YZ{Make this part concise. (Just say this is a continuation of the earlier STLC example)}\EDJreply{Done}
\begin{lstlisting}[language=Coq,  escapeinside={@}{@}]
Family STLC.
  FInductive tm : Set := ... 
  FInductive value : tm -> Prop := ... 
  Closing Fact value_not_app : forall x y, ~ value (tm_app x y) 
      by { intros x y H; inversion H; eauto }.
  Fail Ltac inv := 
    match goal with 
    | [h : value (tm_app _ _) |- ] => destruct (value_not_app _ _ h)
    (* Fail: value, tm_app, value_not_app unfound *)
    end. 
  MetaData tactic1.
    Ltac inv := (* ... the same definition as above ... *)
  EndMetaData.
  (* ... STLC example continue ... *)
EndFamily.
\end{lstlisting}  
\caption{Example for Global Reasoning and MetaData, using in STLC example}\label{fig:plugin-example-global-reasoning-meta-data}
\end{figure}


% Our proposal is to allow global reasoning by providing "Closing Fact" command. Ultimately, "Closing Fact" is asserting a constraint upon all the
% possible extension of the surrounding family and claiming that a proof script
% can solve the constraints.\YZ{This is a good one-liner that summarizes the intended usage scenarios of "Closing Fact". Should probably bring it out earlier.}\EDJreply{I put it here as you suggested but I think this one-liner at here will cause more confusion because it is too abstract.}
% The assertion will only be verified when the whole family is closed and compiled into a module.

% % We provide "Closing Fact" as a command to
% % carry out this global reasoning, attached with a proof script and proof
% % term. This "Closing Fact" will look like an assertion during the
% % construction of the enclosing family. This assertion will only be
% % verified when the whole family is closed and compiled into a module,
% % with the help of the attached proof script and proof terms.

% Here we show the example to prove "value_not_app" in
% \cref{fig:plugin-example-global-reasoning-meta-data}, the script inside
% curly brackets after "by" is the attached proof script that will be
% executed upon "EndFamily", which triggers the compilation of the family
% being defined.
% Since we are dealing with a concrete (vanilla) inductive type during compilation,
% we can use Coq's "inversion" tactic to complete this proof.
% This "Closing Fact", as an assertion, will have no problem being
% inherited, but the proof script will be executed every time a derived
% family is compiled.

% The downside is that "Closing Fact" cannot immediately check the correctness of the
% attached proof term/proof script because this verification only happens when we
% finished defining a family, and thus a bit harder to handle.\YZ{'harder' in what way? I suppose you mean the "Fail Ltac" in Figure 5?}\EDJreply{The proof script cannot be run immediately to reply the user whether the proof script is even correct. This is bad engineering to some extent. In those bad cases, because error only happens at the the very end ``EndFamily'' , the user will need to go back to the place of Closing Fact to adjust the proof script, or to adjust other parts. I am not sure if early check is possible to be done (engineering-wise) I think it is possible (imagine I just do an early compilation, i.e. only compile part of the family to the point closing fact is used) but it is a big engineering difficulty for me.}
% We expect it to be used only sporadically for maintainability, and as an alternative to using "FRecursor". This is called global reasoning because this is a post-hoc reasoning when the family as a \textit{whole} is compiled into a module.



% However, to verify and prove the recursor and partial recursor and their
% computational rules (as in \ref{chg:definition-relevant-reasoning}), our plugin has to
% compile the extensible family into a non-extensible Coq Module and define
% the recursors by instantiating the content of Coq module with concrete
% inductive types. We provide "Closing Fact" as a command to carry out this global reasoning, 

% This style of ``Global Reasoning'' says that we can prove the
% corresponding theorem only when we know the ``full picture'', in particular,
% all the constructors of an inductive type. 
% We provide "Closing Fact" as
% a command to carry out this global reasoning

%\YZ{Are recursors and partial recursors automatically generated and proved by the plugin, or must they be asserted and proved by the programmer?}\EDJreply{(1) Closing Fact is not only used to generate recursor and partial recursor. I think the intro of this section is just giving people wrong impression. So I rewrite the whole section now. (2)  Recursors and Partial recursors are possible to be automatically generated but my plugin didn't implement it. Current stage is that user only need to assert them. Nobody has to prove it (plugin will prove it). Here when I say the user has to provide proof script, it means for arbitrary proposition if the user want to carry out global reasoning.}

% ---we can attach to "Closing
% Fact" either (proof) terms or Coq's proof script, then only when
% compilation to module happens, is the term/proof script
% type-checked to see if a "Closing Fact" statement can be proved. Note
% that, "Closing Fact" cannot immediately check the correctness of the
% term/proof script because this verification only happens when we
% finished defining a family, and thus a bit harder to handle.

% This "Closing Fact" also can ease us from the necessity to let plugin
% generate proof for recursors and partial recursors and their
% computational rules (as in \ref{chg:definition-relevant-reasoning}), and thus making
% plugin development easier.
%\YZ{This reads like a bad excuse: the programmer does not care about if the plugin development is easy. Question is if it makes the programmer's life easier.}\EDJreply{You are right. It does sound like a bad excuse. I rewrite the whole section now. Closing Fact is initially used when the user wants global reasoning instead of extensible proof by induction, for example, the inversion lemma. \\ It just happens that they can be used by me so that I can postpone generation of these rule plugin (which is hard for me to do). This paragraph is just me being honest that my plugin is not mature enough to generate everything.}
% For example, the computation rule for a
% concrete recursor like "B.neg" and partial recursors in
% \cref{fig:plugin-example2} can be directly asserted by using "Closing
% Fact" and their proofs verified later during compilation.
%\YZ{What is the alternative if not using "Closing Fact"?}\EDJreply{(1) For partial recursor and recursor, that should be generated by plugin (2) For general global reasoning, I don't know. There must be some other ways in the literature. }\YZ{Still not entirely clear to me what is gained and what is lost by using "Closing Fact".}\EDJreply{Please check.}
% This ``Global Reasoning'' will be again useful in later examples.


\subsection{Implementation Limitation}
We list some of the current limitation of our implementation. We believe some of these limitations can be remedied by more engineering efforts.
\begin{itemize}
  \item We don't support the full power of inductive facility in Coq for extensible inductive type---concretely, we don't support mutual inductive type, notation for inductive type and parameters in the inductive type (we only support indexed inductive type right now)
  \item We don't support mutual fixpoint definition
  \item We don't support incorporating "Hint" Vernacular commands, and thus the users cannot specify the database hints
  \item Unicode (i.e. "Require Import Coq.Unicode.Utf8.") is not compatible with our plugin 
  \item We don't consider the compatibility of universe polymorphism mechanism with our plugin
  \item We only support automatic generation of partial recursor and the computation information for non-indexed inductive type 
\end{itemize}

\fi