Approaches to modular mechanization or proof reuse exist,
with different focuses and trade-offs.

\noindentparagraph{Encodings based on product lines or DTC.}

\citet{delaware2011} engineer product lines of theorems and proofs built
from feature modules. Feature composition is manual, which seems to have
motivated later approaches based on \ac{DTC}~\cite{dtc}.
%\rB{ define abbreviation DTC at first use?}

%DTC is as a source of inspiration for may approaches
%\cite{delaware2013,delaware2013b,schwaab2013modular,keuchel2013generic,forsta2020}.
The original DTC encoding requires type-level general recursion that
fails the strict positivity check imposed by proof assistants including Coq and Agda.
\citet{delaware2013} introduce \ac{MTC}:
it overcomes the problem by using Church encodings for
data types and using Mendler-style folds for evaluation,
though it requires \lsti{Set} impredicativity.
Feature composition is automated through heavy use of type classes.
The framework is implemented as a Coq library.
\citet{schwaab2013modular} adapt DTC to Agda by considering a restricted
class of functors that admit least fixed points.
\citet{keuchel2013generic} use datatype-generic programming techniques
for the underlying representation of type-level fixed points and
avoid \lsti{Set} impredicativity.
%\Citet{vdr2022intrinsically} mechanize intrinsically-typed
%target language based on container and signature \cite{altenkirch2015container}. 
%Thanks to the nature of intrinsically-typed abstract syntax trees (ASTs) and Agda's advanced unification mechanism, the authors were able to eliminate a significant amount of code that dealt with "impossible" cases.
%\YZ{What can we say about Intrinsically-Typed Definitional Interpreters a La Carte?}\EDJreply{My understanding is,
% it is really an intrinsic typed version of MTC. It uses different set of tools (like containers (a la altenkirch, can be considered as an alternative of W-type) and stuff) but it is still a design pattern paper.
% But good news is, container is closer to Wtype(inductive type) than the endofunctor encoding (classic MTC approach)}\EDJreply{I also add one sentence after Coq a la carte. You can also check... I will add bib later. Interestingly, Intrinsic type doesn't refer Coq a la carte.}

All of these approaches are largely extra\-linguistic, in that they work
within the confine of the language offered by a proof assistant, which
comes with trade-offs.
On the one hand,
they can be conveniently distributed as libraries, and the encoding can
be more easily adapted for new purposes.
For example, MTC has been applied to implementing
composable program adverbs~\cite{liwei2022} and been adapted to allow
feature extensions, such as reference cells and exceptions, that require type changes
\cite{delaware2013b,vdr2022intrinsically}.

On the other hand, the extra\-linguistic nature of the approaches tends
to lead to non-idiomatic code and offset their user-friendliness.
In particular, because data types have to be encoded (rather than expressed
through natively supported inductive types), the resulting code can be
obtuse at first blush, making the programming style inaccessible to non-experts.
In addition, extra programmer effort may be required, such as
having to manually prove additional well-formedness conditions. %on the algebras\EDJ{What is this `algebra'? I mean if it is a very frequent word to the audience  familiar with DTC then removing explanation is fine.}.

\citet{forsta2020} introduce \emph{Coq à la carte}.
It still follows DTC, but rather than embracing DTC's use of generic
fixed points, it considers specific instantiations instead.
The resulting mechanism appears more streamlined than prior \emph{à la
carte} approaches particularly for its extensive tool support for generating boilerplate code.
%
But even with the tool support, components (e.g., "substlem") of
individually developed feature extensions have to be composed separately
by invoking the tool.
%The approach does not seem to fully address the expression problem,
%as \citet{liwei2022} observe, in that further extending a composed
%mechanization causes (a small amount of) boilerplate code that glues definitions
%to be regenerated.

Our approach addresses the expression problem by extending the
linguistic facilities offered by a proof assistant.
Families, in particular, offer an organizational advantage.  They allow
grouping and coevolving related types, functions, and proofs without
explicit parameterization;
all further-bindable fields are automatically extensibility hooks.
Because family polymorphism does not require explicit parameterization
or complex encodings,
the resulting programming experience and code are accessible to the
working Coq programmer.
The more OO aspects of family polymorphism, such as the ability to
use families as mixins and the ability to grow a series of mechanized
languages in integral increments, also facilitate extensibility and
reuse
with minimal programmer effort.

\noindentparagraph{Proof reuse and proof repair.}

\citet{boite2004proof} addresses proof reuse specifically in response
to inductive types extended with new constructors. Proof reuse is
via a tactic that adapts the original proof to the extended inductive
type while generating proof obligations, so rechecking of proof terms is
entailed.
The design requires distinct names for a base inductive type and its
extensions (including distinct names for constructors), while \Lang allows names to be late bound.

\citet{mulhern2006proof} introduces a heuristic approach that allows proofs
for multiple small languages to be combined to yield proofs for
composite languages, as long as the proof structure follows the same
pattern.
%
\citet{johlut2004} enable proof reuse in Isabelle by adapting theorems
from one setting for reuse in another: proof terms are transformed
by first explicitly stating all assumptions, and then abstracting over
function symbols and type constants.

Pumpkin Pi~\cite{ringer2021pumpkin} is a Coq plugin that helps
repair proofs broken by changes in type definitions.
Its decompiler from proof terms to proof scripts prioritizes suggesting
useful tactics over soundness.
While Pumpkin Pi focuses on refactoring existing proofs in response to
changed definitions,
our solution can be viewed as an effort to preempt refactoring by
enabling the programmer to write code that has built-in hooks for
future extensions.

\noindentparagraph{The expression problem.} Solutions abound.
% \EDJ{I have been meaning to ask but is this ``abound'' you intentionally put outside of the boldfont?}\YZreply{Yes}
Almost all involve some form of either explicit or implicit parameterization
as extensibility hooks.
Our approach is the first that applies family polymorphism~\cite{ernst2001family},
mostly seen in OO languages,
to the context of mechanized proofs.
%Families in \Lang are second-class, in that they are not considered
%terms by themselves.
%Families as modeled in \TT (namely linkages) are first-class and thus more flexible.\EDJ{A more unified way is to consider Coq's Family (our surface syntax) second class corresponds to Inheritance Judgement/Linkage Transformer (secondclass in type theory). Linkage transformer is the helper function to construct first class linkage easily. Even non-derived (surface syntax) family can be considered in this way because they are just inheritance on empty family.}
%The distinction corresponds to class-based vs.\ object-base family
%polymorphism as summarized before~\cite{cdnw07-tribe}.

\citet{bac2006} address the expression problem for a core subset of Standard~ML
by combining explicitly coded open recursion with a design that
allows pattern-matching cases to be defined separately and combined
later.
Our \lsti{FRecursion} and \lsti{FInduction} commands achieve a similar
functionality, with families making open recursion implicit and
bestowing organizational power.

\noindentparagraph{ML-style modules}\!\!, like families, are a modularity mechanism,
but with a focus on abstraction rather than extensibility.
Both \TT and the module system of \citet{stohar2000} use singletons to model and
control the propagation of definitions.
\mbox{MixML} \cite{rosdre2013} integrates mixins into ML and handles the
idiosyncrasies of ML modules,
while our work supports mixins in the presence of extensible inductive types.



\ifShowOldWriting

\textbf{Modular Mechanized Metatheory: Linguistic Approach} We group the work that achieves modular mechanized metatheory by extending surface syntax of proof assistants to provide better abstraction as \textit{linguistic approach}. 

The closest work to ours might be \citet{boite2004proof}--they try to support \textit{extended} inductive type and related proof reuse in Coq. This biggest difference lies in our (incomplete) adaption of Family Polymorphism -- apart from supporting proof reuse and extension by inheritance, we also support implementation swapping using overriding, and we also argue that Family Polymorphism can provide a better and cleaner way of organizing mechanized metatheory development a well as solving related expression problem. The other difference lies in the handling of the extension of the inductive type. In our paradigm, the discussion of \textit{extensible} inductive type and \textit{partial recursor} is unavoidable, which characterizes all possible extension of a given inductive type. In their work, without using Family to organized the code, the extended inductive type can be considered as a different inductive type than the original one, possibly leading to more expression problem~\cite{wadler-ep}. The situation is similar for \citet{levin2003tinkertype}---they aim for modular manipulation of metatheory but didn't consider expression problem.  What's more, our implementation is based on Coq plugin and is thus potentially more friendly to Coq community.
% Another similar work is proof weaving \citep{mulhern2006proof} but I don't see a evaluation section in that paper


\textbf{Modular Mechanized Metatheory: Semantic Approach} We group the work using the semantic as encoding (e.g. Church Encoding, datatype-generic programming) to achieve certain design patterns without modify the surface syntax of the proof assistants as \textit{semantic approach}, that includes \citet{delaware2013,forsta2020,liwei2022,schwaab2013modular, keuchel2013generic} and so on. Their advantages are various: their techniques are more general and thus can be applied across different (dependent-typed) proof assistants and their technique can be easily distributed like a library and thus doesn't need to alter the core of the proof assistants. The downsides are mainly about the threshold: they are not friendly for the junior Coq development and these indirect encoding of datatype might lead to worse readability and accessibility compared to the direct style like ours.

\fi